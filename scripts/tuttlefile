file://List_of_S%26P_500_companies.html <- https://en.wikipedia.org/wiki/List_of_S%26P_500_companies ! download

file://../data/constituents.csv <- file://List_of_S%26P_500_companies.html ! python
    from bs4 import BeautifulSoup
    import csv
    from os import mkdir
    from os.path import exists, join
    datadir = join('..', 'data')
    if not exists(datadir):
        mkdir(datadir)
    source_page = open('List_of_S%26P_500_companies.html').read()
    soup = BeautifulSoup(source_page, 'html.parser')
    table = soup.find("table", { "class" : "wikitable sortable" })

    # Fail now if we haven't found the right table
    header = table.findAll('th')
    if header[0].string != "Ticker symbol" or header[1].string != "Security":
        raise Exception("Can't parse wikipedia's table!")

    # Retreive the values in the table
    records = []
    rows = table.findAll('tr')
    for row in rows:
        fields = row.findAll('td')
        if fields:
            symbol = fields[0].string
            name = fields[1].string
            sector = fields[3].string
            records.append([symbol, name, sector])
   
    header = ['Symbol', 'Name', 'Sector']
    writer = csv.writer(open('../data/constituents.csv', 'w'), lineterminator='\n')
    writer.writerow(header)
    # Sorting ensure easy tracking of modifications
    records.sort(key=lambda s: s[1].lower())
    writer.writerows(records)    

    
file://../data/constituents-financials.csv <- http://download.finance.yahoo.com/d/quotes.csv?s=MMM&f=l1yreb4jkj1j4p5p6, file://../data/constituents.csv ! python
# NB : adding http://download.finance.yahoo.com/d/quotes.csv?s=MMM&f=l1yreb4jkj1j4p5p6 to the input resources 
# is a brutal way to force invalidation of constituents-financials.csv, in order to follow the regular 
# change of the yahoo API results
	
    import urllib
    import csv

    def correctToBillions(item):
        if item.endswith('M'):
            return float(item[:-1]) / 1000
        elif item.endswith('B'):
            return item[:-1]
        else:
            return item

    items = [
        ['l1', 'Price'], # strictly this is ask price
        ['y', 'Dividend Yield'],
        ['r', 'Price/Earnings'],
        ['e', 'Earnings/Share'],
        ['b4', 'Book Value'],
        ['j', '52 week low'],
        ['k', '52 week high'],
        ['j1', 'Market Cap'],
        ['j4', 'EBITDA'],
        ['p5', 'Price/Sales'],
        ['p6', 'Price/Book']
    ]
    params = ''.join([ x[0] for x in items ])

    url = 'http://download.finance.yahoo.com/d/quotes.csv?'
    edgar = 'http://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK='


    reader = csv.reader(open('../data/constituents.csv'))
    outrows = [ row for row in reader ]
    symbols = [ row[0] for row in outrows[1:] ]

    outrows[0] += [ item[1] for item in items ] + ['SEC Filings']
    
    for idx in range(0,len(symbols),20):
        query = url + 's=' + '+'.join(symbols[idx:idx+20]) + '&f=' + params
        fo = urllib.urlopen(query)
        tmpcsv  = csv.reader(fo)
        rows = [ row for row in tmpcsv ]
        for count, row in enumerate(rows):
            realidx = idx + count + 1
            # change n/a to empty cell
            row = [ x.replace('N/A', '') for x in row ]
            # market cap and ebitda have 'B' or 'M' in them sometimes
            row[7] = correctToBillions(row[7])
            row[8] = correctToBillions(row[8])
            # add the edgar link
            row.append(edgar + symbols[realidx-1])
            outrows[realidx] = outrows[realidx] + row
        print('Processed: %s rows' % (idx + 20))

    fo = open('../data/constituents-financials.csv', 'w')
    writer = csv.writer(fo, lineterminator='\n')
    writer.writerows(outrows)
    fo.close()
